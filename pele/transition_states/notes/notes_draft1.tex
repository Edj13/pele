\documentclass[a4paper]{article}

%\usepackage[numbers]{natbib}
\usepackage[top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{amsmath}
%\bibliographystyle{naturemagurl}
%\bibliographystyle{plainnat}


\title{Notes on finding the lowest eigenvector}
\author{Jacob Stevenson}
\date{\today}

\begin{document}
\maketitle


This describes a method for estimating the eigenvector of a Hessian matrix $H_{ij} = \partial E(\vec{x}) / (\partial{x_i}\partial x_j)$ with the lowest eigenvalue at a given point.  It is assumed that gradients $g_i = \partial E(\vec{x}) / \partial x_i$
are available, but the exact Hessian is not.
The lowest eigenvector is found by estimating the curvature
at a give point using finite differences and then using an optimization
algorithm to minimizing that function.  At a point $\vec{x}$ the estimate
for the lowest eigenvector is $\vec{v}$.  The curvature is estimated
by the the central differences formula, which is accurate up to order $\delta^2$ where $\delta$ is a small parameter.
\begin{equation}
\mu(\vec{x}, \vec{v}) = \frac{1}{2 \delta} 
\left[ \vec{g}(\vec{x} + \delta \vec{v}) - 
\vec{g}(\vec{x} - \delta \vec{v}) \right] \cdot \vec{v}
\end{equation}
If N is the dimension of the original space then this is an optimization problem in N-1 dimensions because of the constraint that $\left|\vec{v} \right| = 1$.

The gradients of the above function can be computed analytically to aid in the optimization.
\begin{equation}
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial v_k} = 
\frac{1}{2 \delta} 
\sum_i \frac{\partial v_i}{\partial v_k}
\left[ g_i(\vec{x} + \delta \vec{v}) 
- g_i(\vec{x} - \delta \vec{v}) \right]
+
\frac{1}{2 \delta} 
\sum_i v_i
\frac{\partial}{\partial v_k}
\left[ g_i(\vec{x} + \delta \vec{v}) 
- g_i(\vec{x} - \delta \vec{v}) \right]
\end{equation}
In the next we use
\begin{equation}
\frac{\partial g_i(\vec{x} + \delta \vec{v})}{ \partial v_k} 
= 
\delta \sum_j
\frac{\partial g_i(\vec{x} + \delta \vec{v})}{ \partial x_j}
\frac{\partial  v_j} { \partial v_k}
= \delta 
\frac{\partial g_i(\vec{x} + \delta \vec{v})}{ \partial x_k} 
\end{equation}
where $\partial v_i / \partial v_k$ is the kroniker delta $\delta_{ik}$.
\begin{equation}
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial v_k} = 
\frac{1}{2 \delta} 
\left[ g_k(\vec{x} + \delta \vec{v}) 
- g_k(\vec{x} - \delta \vec{v}) \right]
+
\frac{1}{2} 
\sum_i v_i
\frac{\partial}{\partial x_k}
\left[ g_i(\vec{x} + \delta \vec{v}) 
+ g_i(\vec{x} - \delta \vec{v}) \right]
\end{equation}
To second order in $\delta$ we can replace the second term with
\begin{equation}
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial v_k} = 
\frac{1}{2 \delta} 
\left[ g_k(\vec{x} + \delta \vec{v}) 
- g_k(\vec{x} - \delta \vec{v}) \right]
+
\sum_i v_i
\frac{\partial g_i(\vec{x})}{\partial x_k}
\end{equation}
We can now use the eigenvalue equation $\vec{v} \cdot H = \mu \vec{v}$ to further simplify the second term
\begin{equation}
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial v_k} = 
\frac{1}{2 \delta} 
\left[ g_k(\vec{x} + \delta \vec{v}) 
- g_k(\vec{x} - \delta \vec{v}) \right]
+
\mu v_k
\end{equation}
or
\begin{equation}
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial \vec{v}} = 
\frac{1}{2 \delta} 
\left[ \vec{g}(\vec{x} + \delta \vec{v}) 
- \vec{g}(\vec{x} - \delta \vec{v}) \right]
+
\mu \vec{v}
\end{equation}

We want to maintain the constraint $\left|\vec{v} \right| = 1$, so we subtract out the parallel component of the derivative
\begin{equation}
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial \vec{v}} - 
\left(
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial \vec{v}} \cdot \vec{v}
\right) \vec{v}
= 
\frac{1}{2 \delta} 
\left[ \vec{g}(\vec{x} + \delta \vec{v}) 
- \vec{g}(\vec{x} - \delta \vec{v}) \right]
+
\mu \vec{v}
-
\left(
\frac{1}{2 \delta} 
\left[ \vec{g}(\vec{x} + \delta \vec{v}) 
-\vec{g}(\vec{x} - \delta \vec{v}) \right] \cdot \vec{v}
+
\mu \right)
\vec{v}
\end{equation}
The first term in the parenthesis on the right is simply $\mu$ which gives us
finally
\begin{equation}
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial \vec{v}} - 
\left(
\frac{\partial \mu(\vec{x}, \vec{v})} {\partial \vec{v}} \cdot \vec{v}
\right) \vec{v}
= 
\frac{1}{2 \delta} 
\left[ \vec{g}(\vec{x} + \delta \vec{v}) 
- \vec{g}(\vec{x} - \delta \vec{v}) \right]
-
\mu \vec{v}
\end{equation}
*This equation is off by a factor of two. The correct answer is twice the above* 



\end{document}
